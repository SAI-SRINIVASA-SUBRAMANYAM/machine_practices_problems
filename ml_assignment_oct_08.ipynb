{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "ERbIM8EDu6_j"
      },
      "outputs": [],
      "source": [
        "#imports\n",
        "\n",
        "import pandas as pd\n",
        "from scipy.stats import linregress\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import math\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. What is Linear Regression?**\n",
        "<br />\n",
        "<br />\n",
        "Ans. Linear regression model is used to estimates the relationship between **one dependent** & **one dependent quantative** variables.\n",
        "<br />\n",
        "<br />\n",
        "Eg: Some examples are...\n",
        "<ol>\n",
        "<li> Soil Erosion Vs water fall</li>\n",
        "<li> Height Vs Shoe size</li>\n",
        "</ol>\n"
      ],
      "metadata": {
        "id": "s9d5jEiRvSmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear regression example:\n",
        "df = pd.DataFrame([ \n",
        "    [165,36.5],\n",
        "    [168, 37], \n",
        "    [172, 39.5],\n",
        "    [174,37.5],\n",
        "    [170, 37.5],\n",
        "    [171,38], \n",
        "    [164,36], \n",
        "    [173, 38.5],\n",
        "    [169, 37],\n",
        "    [163, 37]\n",
        "    ], columns=['Height', 'ShoeSize'])"
      ],
      "metadata": {
        "id": "-Jhc0-2YxPPd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.corr()\n",
        "# Positive correlation: If there is increase in the x (dependent) value,\n",
        "#  increase in the y (independent) value.\n",
        "# Negative/Inverse correlation: If there is decrease in the x (dependent) value,\n",
        "#  decrease in the y (independent) value."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        },
        "id": "yuKSPF-Ox9yi",
        "outputId": "42e2276c-2aaf-4bb6-e67c-1fa04d3a75e7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            Height  ShoeSize\n",
              "Height    1.000000  0.741125\n",
              "ShoeSize  0.741125  1.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-18d2eabc-7d7e-4fda-a8ec-1e71c7f20b0b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Height</th>\n",
              "      <th>ShoeSize</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Height</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.741125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ShoeSize</th>\n",
              "      <td>0.741125</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-18d2eabc-7d7e-4fda-a8ec-1e71c7f20b0b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-18d2eabc-7d7e-4fda-a8ec-1e71c7f20b0b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-18d2eabc-7d7e-4fda-a8ec-1e71c7f20b0b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Formulas:\n",
        "# Slope(m): n * (E(x)(y) - E(x)E(y)) / n * (E(x) * E(y))\n",
        "# intercept(b): ( E(y) * m(E(x)) ) / n\n",
        "\n",
        "lin_reg = linregress(df['Height'], df['ShoeSize'])\n",
        "print(\"result\", lin_reg)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahEZJgfS0QG5",
        "outputId": "22c760ed-858b-4aca-a756-f01cc865ff15"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "result LinregressResult(slope=0.19525959367945825, intercept=4.470654627539503, rvalue=0.7411253770427022, pvalue=0.014178951245427696, stderr=0.06253677280900707, intercept_stderr=10.564921013099163)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculation of given Height, get shoe size\n",
        "# Formula: y = m x + c\n",
        "# m = slope\n",
        "# c = intercept\n",
        "# x = given height to be calculated\n",
        "\n",
        "given_height = 200\n",
        "print(lin_reg.slope * given_height + lin_reg.intercept)\n",
        "# Resultant is the approx shoe size/value with some residual"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J5MOPluA04PR",
        "outputId": "650b1bcd-f106-45ad-beca-df3288273d79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "43.52257336343115\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plotting graph\n",
        "# A. Scatter plot\n",
        "sns.scatterplot(data=df, x='Height', y='ShoeSize')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "8kB07VXu1h6N",
        "outputId": "a787a27d-6bc6-43f9-e1a4-d2d341d4c5ea"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7feba750ccd0>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# B. Regression plot\n",
        "\n",
        "sns.regplot(data=df, x='Height', y='ShoeSize')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "yk1vvXSV2Ygb",
        "outputId": "15233753-a6be-40b2-90fe-fe9aa71c1bfe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7feba6f293d0>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "MqIYZsZ6M6YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. How to Calculate the error in linear regression?**\n",
        "<br />\n",
        "<br />\n",
        "Ans. A difference between the predicted value and actual value is called **residual**. One of main important assumption of regression analysis is the normal distribution of residual with **mean is equal to 0**, ie., residual must be positive and negative.\n",
        "<br/>\n",
        "<b>Formula</b> RSS (Residual Sum of Squares): \n",
        "<br/>\n",
        "<h2>Calculating Residuals</h2>\n",
        "Knowing that\n",
        "i = y(i) − ^y(i)\n",
        "<br />\n",
        "^y(i) = m x + b"
      ],
      "metadata": {
        "id": "ByfsPqZv2m5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Demo calculating residual value\n",
        "\n",
        "given_height = 165\n",
        "actual_shoe_size = 36.5\n",
        "r_shoe_size = lin_reg.slope * given_height + lin_reg.intercept\n",
        "print(\"Residual value:\", actual_shoe_size - r_shoe_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PxHKbWcD2aPa",
        "outputId": "38124b29-d38e-430c-fa6d-77549bf8ca44"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Residual value: -0.18848758465011173\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "HZnOi94aM8wv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Difference between Loss and Cost function?**\n",
        "<br/>\n",
        "Ans. \n",
        "<br/>\n",
        "**Loss Function:** The Loss function quantifies how much a model function's prediction ^y = f(x) deviates from the ground truth y = y(x), we calculate loss on the single object in the training or test sets\n",
        "<br/>\n",
        "Examples:\n",
        "<ul>\n",
        "<li> MSE: Mean Square Error</li>\n",
        "<li> MAE: Mean Absolute Error</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "**Cost Function:** The cost function measure the model's error on a group of objects.\n",
        "<br/>\n",
        "Examples:\n",
        "<ul>\n",
        "<li> Average Square Loss.</li>\n",
        "</ul>\n",
        "\n",
        "**Object Functions:** The object these functions is not to minimize the cost/loss over training data, is to avoid **overfitting**"
      ],
      "metadata": {
        "id": "b0QZ4Ak8A_Jd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "TiZURb7UL_HV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Difference between MSE & MAE & RMSE functions?**\n",
        "\n",
        "**Description:** \n",
        "<ul>\n",
        "<li>MAE (Mean Absolute Error): It represents the differences the average of the absolute difference betweeen the actual and predicted values in the dataset</li>\n",
        "<li>MSE (Mean Square Error): It represents the average of the squared difference between original and predicted values in the dataset.</li> \n",
        "<li>RMSE (Root Mean Square Error): It is the square root of mean square error i.e., it measures the standard deviation of residuals</li>\n",
        "</ul>\n",
        "\n",
        "**Examples:**"
      ],
      "metadata": {
        "id": "8Mfi6FVWGxbj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# MAE(Mean Absolute Error):\n",
        "# 1/N E(i=1 to N) |y(i) - ^y|\n",
        "\n",
        "print(\"Mean Absolute Error (MAE), value: \", mean_absolute_error(df['Height'], df['ShoeSize']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkbU4MVFAiGP",
        "outputId": "b5165e8b-0d17-43e4-f3de-d0bca97c7310"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error (MAE), value:  131.45\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# MSE(Mean Square Error):\n",
        "# 1/N E(i=1 to N) (y(i) - ^y)**2\n",
        "\n",
        "print(\"Mean Squared Error (MSE), value: \", mean_squared_error(df['Height'], df['ShoeSize']))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuMqqrwsO_P9",
        "outputId": "2539d344-8767-4423-8c3a-e9b3b9888188"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (MSE), value:  17288.125\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# RMSE(Root Mean Square Error):\n",
        "# Square root (1/N E(i=1 to N) (y(i) - ^y)**2)\n",
        "MSE = mean_squared_error(df['Height'], df['ShoeSize'])\n",
        "print(\"Root Mean Squared Error (RMSE), value: \", math.sqrt(MSE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6z9g0z1ZPda0",
        "outputId": "7af1e005-4d91-4814-dccd-38b1472500c2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Root Mean Squared Error (RMSE), value:  131.48431465387802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "0rCisCXzQWPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain how `Gradient Descent` works in `Linear Regression`** ?\n",
        "\n",
        "Ans.\n",
        "\n",
        "**Simple definition:** A grident simply measures the change in all weights with regard to the change in error ie., gradient as the slope of a function. \n",
        "\n",
        "The higher gradient, the steeper the slope and the faster a model can learn. But if the slope is zero, the model stops learning\n",
        "\n",
        "**Gradient descent used to optimze the following:**\n",
        "<ul>\n",
        "<li><b>Linear Regression:</b> Optimizing interpect and slope</li>\n",
        "<li><b>Logistic regression:</b> Optimizing the squiggle</li>\n",
        "<li><b>t-sne:</b> Optimizing a cluster</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "GCGo4WKOU6oH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example:\n",
        "# Basic formula:\n",
        "# ^y = mx + b\n",
        "# loss = (y - ^y) ** 2 /N\n",
        "\n",
        "numvals = 50\n",
        "\n",
        "# initial values of slope and intercept\n",
        "m, b = 0, 0 \n",
        "\n",
        "# initial learning rate is 0.01\n",
        "learning_rate = 0.01\n",
        "\n",
        "x = np.sort( 5 * (np.random.random(numvals) - 0.5))\n",
        "y = w * (x + x**2) + np.random.normal(0, 0.1, numvals)\n",
        "\n",
        "print('X coordinates', x)\n",
        "print('y coordinates', y)"
      ],
      "metadata": {
        "id": "6Hy9ae9WQL4l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b7497d2-8022-4ab7-e984-a8ff9f39ce47"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X coordinates [-2.42762553 -2.12333046 -2.08845191 -2.07467014 -2.07037784 -1.94919672\n",
            " -1.8731352  -1.55046668 -1.36035126 -1.29891549 -1.2628666  -1.2137046\n",
            " -1.04217106 -1.02307476 -0.93647488 -0.82466348 -0.80736693 -0.79815056\n",
            " -0.63186853 -0.4421531  -0.43036788 -0.42141995 -0.33421099 -0.06613712\n",
            "  0.19389849  0.2290804   0.31323032  0.50789565  0.50941377  0.59877357\n",
            "  0.61233613  0.62976403  0.99144864  0.99214614  1.09602753  1.27049395\n",
            "  1.54802706  1.73934376  1.77630804  1.88291081  1.89524184  1.94561474\n",
            "  1.95509858  2.0091827   2.27455716  2.27623994  2.31468997  2.32858835\n",
            "  2.39747781  2.48165976]\n",
            "y coordinates [ 1.7804138   1.20005757  1.07843562  1.14950644  1.01555784  0.8834136\n",
            "  0.82920046  0.39357633  0.12912198  0.4051091   0.15830918  0.14477308\n",
            "  0.12651487  0.0638972   0.12484289 -0.1207092   0.02703247 -0.12851859\n",
            " -0.24673724 -0.01383041 -0.14285991 -0.16367511 -0.07358105 -0.03500178\n",
            "  0.04350097  0.17904849  0.25698312  0.28254385  0.34357351  0.57909133\n",
            "  0.67358818  0.40670508  0.92291769  0.8794482   1.24219261  1.38207991\n",
            "  1.97155073  2.40589152  2.49666631  2.65251747  2.78094505  2.88176161\n",
            "  2.8730119   3.02636479  3.73142407  3.60344682  3.90499906  3.92047787\n",
            "  4.0208667   4.29281685]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting scatter graph\n",
        "\n",
        "plt.figure(figsize=(7, 7))\n",
        "plt.scatter(x, y)\n",
        "plt.xlabel('x', fontsize=20)\n",
        "plt.ylabel('y', fontsize=20)\n",
        "plt.title('Data', fontsize=25)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "1xzbM8qw6rPb",
        "outputId": "a0e6099a-2635-4309-ea65-87e5f116e3bf"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Data')"
            ]
          },
          "metadata": {},
          "execution_count": 42
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 504x504 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# basic gradient descent functional approach\n",
        "\n",
        "def descend(x, y, m, b, learning_rate): \n",
        "    dldw = 0.0 \n",
        "    dldb = 0.0 \n",
        "    N = x.shape[0]\n",
        "    # loss = (y-(wx+b)))**2\n",
        "    for xi, yi in zip(x,y): \n",
        "       dldw += -2*xi*(yi-(m*xi+b))\n",
        "       dldb += -2*(yi-(m*xi+b))\n",
        "    \n",
        "    # Make an update to the w parameter \n",
        "    m = m - learning_rate*(1/N)*dldw\n",
        "    b = b - learning_rate*(1/N)*dldb\n",
        "    return m, b\n"
      ],
      "metadata": {
        "id": "0Wfaq70z7Wba"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Iteratively make updates\n",
        "for epoch in range(200): \n",
        "    m, b = descend(x,y,m,b,learning_rate)\n",
        "    yhat = m*x + b\n",
        "    loss = np.divide(np.sum((y-yhat)**2, axis=0), x.shape[0]) \n",
        "    print(f'{epoch} loss is {loss}, paramters m:{m}, b:{b}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABwmZvYV_WOt",
        "outputId": "62ea807f-3c01-4274-a06d-c4ef70026a55"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "1 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "2 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "3 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "4 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "5 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "6 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "7 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "8 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "9 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "10 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "11 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "12 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "13 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "14 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "15 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "16 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "17 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "18 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "19 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "20 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "21 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "22 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "23 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "24 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "25 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "26 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "27 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "28 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "29 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "30 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "31 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "32 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "33 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "34 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "35 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "36 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "37 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "38 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "39 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "40 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "41 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "42 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "43 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "44 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "45 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "46 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "47 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "48 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "49 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "50 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "51 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "52 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "53 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "54 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "55 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "56 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "57 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "58 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "59 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "60 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "61 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "62 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "63 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "64 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "65 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "66 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "67 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "68 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "69 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "70 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "71 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "72 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "73 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "74 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "75 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "76 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "77 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "78 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "79 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "80 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "81 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "82 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "83 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "84 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "85 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "86 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "87 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "88 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "89 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "90 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "91 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "92 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "93 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "94 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "95 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "96 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "97 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "98 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "99 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "100 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "101 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "102 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "103 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "104 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "105 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "106 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "107 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "108 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "109 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "110 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "111 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "112 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "113 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "114 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "115 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "116 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "117 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "118 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "119 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "120 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "121 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "122 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "123 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "124 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "125 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "126 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "127 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "128 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "129 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "130 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "131 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "132 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "133 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "134 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "135 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "136 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "137 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "138 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "139 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "140 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "141 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "142 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "143 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "144 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "145 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "146 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "147 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "148 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "149 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "150 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "151 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "152 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "153 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "154 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "155 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "156 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "157 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "158 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "159 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "160 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "161 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "162 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "163 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "164 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "165 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "166 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "167 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "168 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "169 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "170 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "171 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "172 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "173 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "174 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "175 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "176 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "177 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "178 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "179 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "180 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "181 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "182 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "183 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "184 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "185 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "186 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "187 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "188 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "189 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "190 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "191 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "192 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "193 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "194 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "195 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "196 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "197 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "198 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n",
            "199 loss is 0.8868453559536975, paramters m:0.6549335408152704, b:1.1056858183977485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "NYMxv3hCAj0P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explain the term intercept?**\n",
        "\n",
        "Ans. The intercept (sometimes called the \"constant\")in a regression model represents the mean value of the response variable when all of the predictor variables in the model are equal to zero.\n",
        "\n",
        "Formula:\n",
        "\n",
        "**Simple linear regression: ŷ = β0 + β1(x)**\n",
        "\n",
        "\n",
        "<ul>\n",
        "<li>ŷ: The predicted value for the response variable</li>\n",
        "<li>β0: The mean value of the response variable when x = 0</li>\n",
        "<li>β1: The average change in the response variable for a one unit increase in x</li>\n",
        "<li>x: The value for the predictor variable</li>\n",
        "</ul>\n",
        "\n",
        "**Multiple Linear Regression: ŷ = β0 + β1(x1) + β2(x2) + β3(x3) + … + βk(xk)**\n",
        "<ul>\n",
        "<li>ŷ: The predicted value for the response variable</li>\n",
        "<li>β0: The mean value of the response variable when all predictor variables are zero</li>\n",
        "<li>βj: The average change in the response variable for a one unit increase in the jth predictor variable, assuming all other predictor variables are held constant</li>\n",
        "<li>xj: The value for the jth predictor variable</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "PJdDBl8-BBBJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "_2SiKcy3GsPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Write all the assumptions for linear regression?**\n",
        "\n",
        "Ans. \n",
        "<ul>\n",
        "<li>Linear relationship</li>\n",
        "<li>Independence</li>\n",
        "<li>Homoscedasticity</li>\n",
        "<li>Normality</li>\n",
        "</ul>\n",
        "\n",
        "\n",
        "**Linear relationship:** There exists a linear relationship between the independent variable, x, and the dependent variable, y.\n",
        "\n",
        "If assumption is voilated:\n",
        "<ul>\n",
        "<li>Apply a non-linear transformation to the independent and/or dependent variable. </li>\n",
        "<li>Add another independent variable to the model.</li>\n",
        "</ul>\n",
        "\n",
        "**Independence:** The residuals are independent. In particular there is no correlation between consecutive residuals in time series data.\n",
        "\n",
        "If assumption is voilated:\n",
        "<ul>\n",
        "<li>Positive serail correlation, consider adding lags of the dependent and/or independent variable to the model. Common examples include taking the log, the square root or the reciprocal of the independent and/or dependent variable</li>\n",
        "<li>Negative serail correlation, check to make sure that none of your variables are overdifferenced.</li>\n",
        "<li>Categorial correlation, consider adding categorial dummy variable to the model.</li>\n",
        "</ul>\n",
        "\n",
        "**Homoscedasticity:** The residuals have constant variance at every level of x.\n",
        "\n",
        "If assumption is voilated:\n",
        "<ul>\n",
        "<li>Transform the dependent variable</li>\n",
        "<li>Redefine the dependent variable</li>\n",
        "<li>Use weighted regression</li>\n",
        "</ul>\n",
        "\n",
        "**Normality:** The residuals of the model are normally distributed.\n",
        "\n",
        "If assumption is voilated:\n",
        "<ul>\n",
        "<li>Verify that any outliers aren't having a huge impact on the distribution.</li>\n",
        "<li>Make sure of no erroneous data</li>\n",
        "<li>Try applying a nonlinear transformation to the independent and/or dependent variable. Common example include taking the log, the square root, or the reciprocal of the independent and/or dependent variable.</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "HP7D3Oo0GhwH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "PIJ1i33LOGsC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Hypothesis testing in Linear Regression?**\n",
        "\n",
        "Ans. \n",
        "<ul>\n",
        "<li>By creating the linear regression model, we are establishing a new truth about the relationship between dependent variable with one or more independent variables.</li>\n",
        "<li>In order to justify the truth, there are needed one or more tests. These tests can be termed as an act of testing the truth using hypothesis testing</li>\n",
        "<li>T-test is used to verify the relationsip between the dependent and independent variables</li>\n",
        "<li>F-test is used to test the linear regression model</li>\n",
        "</ul>\n",
        "\n",
        "Steps to perform **Hypothesis tests & Linear Regression Models**\n",
        "\n",
        "<ul>\n",
        "<li>\n",
        "Formulate null and alternative hypothesis:\n",
        "<ul>\n",
        "<li><b>T-tests:</b> Thus, the null hypothesis is set that there is no relationship between independent variable and the predicted variable\n",
        "</li>\n",
        "<li><b>F-tests:</b> The null hypothesis is that the linear regression model doesnot exists.\n",
        "</li>\n",
        "</ul>\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "Determine the test statistics:\n",
        "<ul>\n",
        "<li>T-test for hypotheses related to individual coefficients</li>\n",
        "<li>F-test for hypothesis for linear regression model</li>\n",
        "</ul>\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "Make decision: Based on the values of t-statistics and f-statistics, the decision regarding the linear regression model is taken\n",
        "</li>\n",
        "\n",
        "<li>\n",
        "Draw conclusions.\n",
        "</li>\n",
        "\n",
        "</ul>"
      ],
      "metadata": {
        "id": "ZaxIIFFtOQw6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "tQA0uDpjSyUq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. How would you decide the importance of variable for the multivariant regression?**\n",
        "\n",
        "Ans. It is defined as a process of involving multiple dependent variables resulting in one outcome. \n",
        "\n",
        "The significance of multivariant analysis is helpful in effectively minimizing the bias.\n",
        "\n",
        "**Objective of multivariant analysis:**\n",
        "<ul>\n",
        "<li>Data reduction or structural simplification</li>\n",
        "<li>Sorting and grouping</li>\n",
        "<li>Investigation of dependence among variables</li>\n",
        "<li>Prediction relationship between variables</li>\n",
        "<li>Hypothesis construction and testing</li>\n",
        "</ul>\n",
        "\n",
        "**Some categories of multivariant analysis:**\n",
        "<ul>\n",
        "<li>Cluster analysis</li>\n",
        "<li>Multiple Logistic Regression</li>\n",
        "<li>Multivariate Analysis of Varaiance</li>\n",
        "</ul>\n",
        "\n",
        "**Model Assumptions:**\n",
        "Normality, Homoscedasticity, linearity and independence(the absence of correlated errors).\n",
        "\n",
        "<b>Example:</b> Based on the season we cannot predict the weather of any given year. Several factors play an important role in predicting the **weather**.\n",
        "\n",
        "\n",
        "**Drawbacks:**\n",
        "<ul>\n",
        "<li>This technique involve the use of complex statistical programs that are very expensive</li>\n",
        "</ul>"
      ],
      "metadata": {
        "id": "hzo_R7KzS4db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "IV7ZNEMEZRyO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q. Difference between R-Square and Adjusted R-Square?**\n",
        "\n",
        "\n",
        "Ans: \n",
        "<ul>\n",
        "<li>R-Square: Residual Sum of Squares.</li>\n",
        "<li>Adjusted R-Square: Adjusted Residual Sum of Squares.</li>\n",
        "</ul>\n",
        "\n",
        "**R-Square:** It is a statistical measure that represents the proportion of the variance for a dependent variable that's explained by an independent variable or variables in a regression model.\n",
        "\n",
        "**Adjusted R-Squared:** It is a modified version of R-squared that has been adjusted for the number of predictors in the model.\n",
        "Adding more independent variables or predictors to a regression model tends to increase the R-squared value, which tempts markers of the model to add even more variables. This is called overfitting and can return an unwarranted high R-Squared value. Adjusted R-squared is used to determine how reliable the correlation is and how much it is determined by the addition of independent variables \n",
        "\n",
        "**Formula:**\n",
        "\n",
        "R Square(Coefficient of determination): \n",
        "\n",
        "1 - (Sum of squares of residuals/total sum of square)\n",
        "\n",
        "<b>Sum of Squares of Residuals(RSS): E(i=1 to n) (y(i) - f(x(i)))**2</b>\n",
        "<ul>\n",
        "<li>y(i): i^th value of the variable to be predicted</li>\n",
        "<li>f(x(i)): predicted value of y(i)</li>\n",
        "<li>n: number of observations</li>\n",
        "</ul>\n",
        "\n",
        "<b>Total Sum of Square(TSS): E(i=1 to n) (y(i) - ^y)**2</b>\n",
        "<ul>\n",
        "<li>n: number of observations</li>\n",
        "<li>y(i): value in a sample</li>\n",
        "<li>^y: mean value of a sample</li>\n",
        "</ul>\n",
        "\n",
        "<b>Adjust R Square: 1-( (1-R**2)(N-1)/(N-K-1) )</b>\n",
        "\n",
        "R**2 is the actual r square value\n",
        "\n",
        "N: Number of data points\n",
        "\n",
        "K: Indepenedent variable\n",
        "\n",
        "**R-Squared Vs Adjusted R-Squared**\n",
        "\n",
        "The Adjusted R-Squared considers and tests different independent variables against the model, where as R-Squared not."
      ],
      "metadata": {
        "id": "SSOB0ndGYDOq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1y9Qhpi7gutq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}